\documentclass{book}
\input{preamble}
\begin{document}
\chapter{Natural Deduction and Inductive Definitions}
\section{Introduction}

As we are going to discuss and reason about properties of various programming
languages, we need a formal way to provide a clear, unambiguous specification of
a programming language. Such a specification tool is itself a language, called a
\emph{meta-language}. While meta-languages do not necessarily correspond to
computer programs, as programming languages do, they do provide a \emph{proof
  calculus} that can be used to write formal proofs about languages and the
programs written in them. The target of these proofs and specifications is
called the \emph{object language}, in order to differentiate it from the
meta-language.

Originally developed for formalising
logics, the \emph{Natural Deduction} of Gentzen\todo{cite} is a simple
meta-language which works well for specifying all aspects of programming
languages, ranging from the grammar and syntax, to static checks, to runtime
behaviour of language constructs.

This chapter will introduce a fragment of natural deduction, as well as
demonstrate a few basic proof techniques, which will be used throughout
the rest of the book. 

\section{Judgements and Inference Rules}

At the heart of natural deduction is the notion of a \emph{judgement}. A
judgement is a mathematical statement that a certain property holds, for
example:

\begin{itemize}
\item \<3 + 4 \times 5\> is a valid arithmetic expression
\item the string \<"aba"\> is a palindrome
\item \<0.43423\> is a floating point value
\item the sum of 3 and 5 is 8.
\item the number 3 is not equal to the number 4
\end{itemize}
To prevent the ambiguity that arises from using natural language, we invent some
more structural notation for judgements, as shown below for the above examples:

\noindent {\renewcommand{\arraystretch}{2}\begin{tabularx}{\textwidth}{cXr}
$\quad\bullet$ &  \<3 + 4 \times 5\> is a valid arithmetic expression  & $\pred{3 + 4 \times 5}{ok}$ \\
$\quad\bullet$ & the string \<"aba"\> is a palindrome & $\pred{\text{\<"aba"\>}}{palindrome}$ \\
$\quad\bullet$ & \<0.43423\> is a floating point value & $\pred{0.43423}{float}$ \\
$\quad\bullet$ & the sum of 3 and 5 is 8. & $3 + 5 = 8$ \\
$\quad\bullet$ & the number 3 is not equal to the number 4 & $3 \neq 4$ \\
\end{tabularx}}
Judgements by themselves would be boring and fairly useless. For example, the
judgment $\pred{3 + 4 \times 5}{ok}$ says that $3 + 4 \times 5$ is a valid
arithmetic expression, but we have no way of proving that any other arithmetic
expression is valid. As there are an infinite number of valid expressions, it
would obviously be impossible to explicitly list them all using simple
judgements. Luckily, the set of valid arithmetic expressions is not a random
collection of objects, but can be systematically defined by so-called
\emph{inference rules}.

Inference rules allow us to combine judgements to obtain new judgements and
have the following form\footnote{The format of rules used in proof theory, for
  example in Gentzen's original Natural Deduction paper, is much more general, 
  but this simple format is sufficient for our purposes}:
\[
\inferrule{J_1\;\; J_2\;\; \ldots\;\; J_n}{J}
\]
where the judgements $J_1$ to $J_n$ are called \emph{premises}, and $J$ is
called a \emph{conclusion}.  This can be read in two directions:
\begin{enumerate}
\item If each judgement $J_1 \dots J_n$ holds, then $J$ holds (Implication).
\item In order to prove $J$, it suffices to prove each of $J_1 \dots J_n$.
\end{enumerate}
An inference rule is not required to have any premises at all. Such inference rules are called
\emph{axioms}. 


As an initial example, we will define the set of natural numbers $\mathbb{N}$. Here our judgement will be that a certain number $n$ is in the set of natural numbers, written $n \in \mathbb{N}$. 

Defining $\mathbb{N}$ by listing all its elements would be equivalent to including an axiom for each number:
\begin{gather*}
\inferrule{\ }{0\in \mathbb{N}} \quad \inferrule{\ }{s(0) \in \mathbb{N}} \quad \inferrule{\ }{s(s(0))\in \mathbb{N}} \quad
\cdots
\end{gather*}
 For  simplicity reasons, we represent each successive number $0,1,2,3,\dots$ as a unary encoding: \<0, s(0), s(s(0)),
s(s(s(0))),\dots\>  ($s$ here stands for \textit{successor}).

Apart from the very first rule, note that all of these rules have the form ${s(x) \in \mathbb{N}}$,
where $x \in \mathbb{N}$ has been established by the previously listed axiom. We can generalise this pattern into a rule of inference, giving us the following definition:
\begin{enumerate}
  \item $\conc{0} \in \mathbb{N}$, and
  \item for all $\meta{x}$, if $\meta{x} \in \mathbb{N}$, then  $\conc{s(\meta{x})} \in \mathbb{N}$.
\end{enumerate}
which can be translated directly into the following two inference rules:
\begin{definition}[Natural Numbers $\mathbb{N}$]
\begin{gather*}
\inferrule{\ }{\conc{0} \in \mathbb{N}}\textsc{Zero} \quad  \quad
\inferrule{\meta{x} \in \mathbb{N}}{\conc{s(\meta{x})} \in \mathbb{N}}\textsc{Suc}
\end{gather*}
\end{definition}
\noindent where $\meta{x}$ can be instantiated to any expression in our object language. 
Here, $\meta{x}$ is an example of a \emph{metavariable}, that is, a variable in our
\emph{meta-language}, which stands for any expression in our \emph{object
  language}. More formally, $\textsc{Suc}$ could be considered as a
\emph{schema} describing an infinite \emph{family} of rules, one for every 
possible substitution for $\meta{x}$. Typically, authors try and syntactically 
distinguish metavariables from concrete symbols in the object language, but it
can still be confusing, particularly if the object language also contains 
variables. To reduce this confusion, we have adopted a simple colour coding 
convention for rules in this book: Metavariables are coloured 
\textcolor{blue}{blue}; syntax from the object language is coloured 
\textcolor{mygreen}{green}; everything else, typeset in black, forms part of the
meta-language syntax.

\section {Deductive Proofs}
\label{sec:Deductive}
As previously mentioned, inference rules work in two ways: we can use them
not just to define when a judgement holds, but also as a means of proof. 

Here are some definitions for finer predicates on
natural  numbers:  $\textsf{\textbf{Even}}$, and its opposite, $\textsf{\textbf{Odd}}$.
\begin{definition}[\sansbold{Even} and \sansbold{Odd}]
\begin{gather*}
\inferrule{\ }{\predc{0}{Even}}\textsc{Zero}\quad 
\inferrule{\predm{x}{Even}}{\predc{s(s(\meta{x}))}{Even}}\textsc{Step}\\
\inferrule{\  }{\predc{s(0)}{Odd}}\textsc{One}\quad 
\inferrule{\predm{x}{Odd}}{\predc{s(s(\meta{x}))}{Odd}}\textsc{Step}'
\end{gather*}
\end{definition}
\noindent Assume we want to prove that the judgement \<\predc{s(s(0))}{Even}\> holds. We
have to look for a rule which has \<\predc{s(s(0))}{Even}\> as a conclusion.
There are no axioms that show this goal directly, but if we \emph{instantiate}
the metavariable $\meta{x}$ to be $\conc{0}$ in the $\textsc{Step}$ rule schema, 
we have the rule:
\[\inferrule{\predc{0}{Even}}{\predc{s(s(0))}{Even}}\textsc{Step}^0\]
Therefore, we know that it suffices to prove $\predc{0}{Even}$ in order to show
our goal, which is easily satisfied by the axiom $\textsc{Zero}$.

Similarly, we can show that \<\predc{s(s(s(s(s(0)))))}{Odd}\> by repeatedly
instantiating the $\textsc{Step'}$ rule until the problem is reduced to the
$\textsc{One}$ axiom. An alternative and often quite convenient way to
write inference proofs is to stack the rules applied to each goal, and draw a
``proof tree'' --- in our example, more a proof stack, since each rule has
just a single premise. 
\[
\inferrule*[Right=Step']{
  \inferrule*[Right=Step']{
    \inferrule*[Right=Step']{
      \inferrule*[Right=One]{\ }{
      \predc{s(0)}{Odd}}}{
      \predc{s(s(0))}{Odd}}
  }{\predc{s(s(s(0)))}{Odd}}
}{
  \predc{s(s(s(s(s(0)))))}{Odd}
}
  \]
It's important to note that our rules of inference have a purely syntactic
basis. Given the rules above, we are not able to prove something like 
\<\predc{2}{Even}\>, even though we can show that \<\predc{s(s(0))}{Even}\>,
and we know that \<\conc{s(s(0))}\> is equivalent to \<\conc{2}\>. We cannot
apply that equivalence since we have no rule which tells us how to do so. 
We are forced to simply manipulate terms mechanically according to the 
given rules.

\newcommand{\mconc}[1]{\texttt{\conc{#1}}}
Let us examine a slightly more interesting example. Suppose we want to define the
language \<\sansbold{M}\> which contains all expressions of properly matched parenthesis
(and no other characters):\footnote{$\epsilon$ represents the empty string}
\[M = \{\conc{\epsilon}, \mconc{()}, \mconc{()()}, \mconc{()()()}, \ldots, \mconc{(())}, \mconc{((()))}, \ldots, \mconc{()(())},\mconc{()()(())}, \ldots \}\] 
Again, let us start by describing the rules in
(semi-)natural language. There are essentially two ways to ``legally'' combine
parentheses: we can either nest them, or concatenate them.
\begin{enumerate}
\item  The empty string (denoted  by $\conc{\epsilon}$) is in \<\sansbold{M}\>
\item  If \<\meta{s_1}\> and \<\meta{s_2}\> are in \<\sansbold{M}\>, then \<\meta{s_1s_2}\> is in \<\sansbold{M}\>
\item   If \<\meta{s}\> is in \<\sansbold{M}\>, then \<\mconc{(}\meta{s}\mconc{)}\> is in \<\sansbold{M}\>
\end{enumerate}
Directly translating into inference rules, we get:

\begin{gather*}
\inferrule{\ }{\predc{\epsilon}{M}}(1) \quad
\inferrule{\predm{s_1}{M}\quad\predm{s_2}{M}}{\predm{s_1s_2}{M}}(2) \quad
\inferrule{\predm{s}{M}}{\pred{\mconc{(}\meta{s}\mconc{)}}{M}}(3)
\end{gather*}
How can we show that  \<\mathtt{\mconc{()(())}}\> is in \<\sansbold{M}\>? As 
we did previously, we check if there is a rule whose conclusion matches the 
judgement we want to infer. If we apply (2), we have to show that both 
$\mconc{()}$ and $\mconc{(())}$ are in $\sansbold{M}$. Since $\mconc{()}=\conc{\texttt{(}\epsilon\texttt{)}}$, 
we can apply (3), and only have to show that $\conc{\epsilon}$ is in
$\sansbold{M}$, which is simply the axiom~(1). By applying the rules~(3) and (1) 
in the same way, we can show that $\mconc{(())}$ is in $\sansbold{M}$ as well, and we 
are done: 

\begin{gather*}
\inferrule*[Right=(2)]{\inferrule*[Right=(3)]
                                  {\inferrule*[Right=(1)]
                                              {\ }
                                              {\predc{\epsilon}{M}}}
                                  {\predc{\texttt{()}}{M}}  
                    \\ \inferrule*[Right=(3)]
                                  {\inferrule*[Right=(3)]
                                              {\ \inferrule*[Right=(1)]
                                                            {\ }
                                                            {\predc{\epsilon}{M}} }
                                              {\predc{\texttt{()}}{M}} }
                                  {\predc{\texttt{(())}}{M}}}
                      {\predc{\texttt{()(())}}{M}}
\end{gather*}
%
The problem is, however, not as straightforward as it seems, because instead
of applying rule~(2), we could as well apply rule~(3):
\begin{gather*}
\inferrule*[Right=(3)]{\inferrule*{?}{\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}
\end{gather*}
As the rule has a single premise, we only have to prove that $\mconc{)(()}$ is
in $\sansbold{M}$. The only rule that's applicable now is rule~(2), and we could apply it in
several different ways resulting in different subgoals:

\begin{gather*}
\inferrule{\inferrule*[Right=(2)]
                                  {\predc{\epsilon}{M} \\ \predc{\texttt{)(()}}{M} }
                                  {\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}(3)
\enskip \mathit{or} \enskip 
\inferrule{\inferrule*[Right=(2)]
                                  {\predc{\texttt{)}}{M} \\ \predc{\texttt{(()}}{M} }
                                  {\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}(3)
\enskip \mathit{or} \enskip
\inferrule{\inferrule*[Right=(2)]
                                  {\predc{\texttt{)(}}{M} \\ \predc{\texttt{()}}{M} }
                                  {\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}(3) \\
\mathit{or} \\
\inferrule{\inferrule*[Right=(2)]
                                  {\predc{\texttt{)((}}{M} \\ \predc{\texttt{)}}{M} }
                                  {\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}(3)
\enskip \mathit{or} \enskip
\inferrule{\inferrule*[Right=(2)]
                                  {\predc{\texttt{)(()}}{M} \\ \predc{\epsilon}{M} }
                                  {\predc{\texttt{)(()}}{M}}}
                      {\predc{\texttt{()(())}}{M}}(3)
\end{gather*}
We can immediately rule out the first and last options because, while they have
one easily provable subgoal (that $\conc{\epsilon}$ is in $\sansbold{M}$),
they get us no closer to proving our overall goal. 
In the second and the fourth application, we end up with the goal $\pred{\mconc{)}}{M}$, but there
is no rule which we can apply to discharge it. This is not that
surprising, since $\sansbold{M}$ should only contain expressions of properly matched
parentheses, and $\mconc{)}$ (as well as $\mconc{)(()}$) is not properly matched. So, by
choosing the wrong rule, or applying the right rule in a wrong way -- for example,
splitting $\mconc{()()}$ up into $\mconc{()(}$ and $\mconc{)}$ --
it is easily possible to end up with goals which
are not actually provable and reach a dead end. In our example, this was not hard
to see. It's also possible that there exists multiple proofs of the same
property. For example, $\pred{\mconc{()}}{M}$ can be shown an infinite number of
ways:

\begin{gather*}
\inferrule{\inferrule*[Right=(1)]
                         {\ }
                         {\predc{\epsilon}{M}} }
           {\predc{\texttt{()}}{M}}(3)
\enskip \mathit{or} \enskip
\inferrule{\inferrule*[Right=(1)]{\ }{\predc{\epsilon}{M}} \\ \inferrule*[Right=(3)]{\ \inferrule*[Right=(1)]
                         {\ }
                         {\predc{\epsilon}{M}} }
           {\predc{\texttt{()}}{M}}}
          {\predc{\texttt{()}}{M}}(2)
\enskip \mathit{or} \enskip
\inferrule{\inferrule*[Right=(2)]{\inferrule*[Right=(1)]{\ }{\predc{\epsilon}{M}} \\ \inferrule*[Right=(3)]{\ \inferrule*[Right=(1)]
                         {\ }
                         {\predc{\epsilon}{M}} }
           {\predc{\texttt{()}}{M}}}
          {\predc{\texttt{()}}{M}} \\ \inferrule*[Right=(1)]{\ }{\predc{\epsilon}{M}} }{\predc{\texttt{()}}{M}}(2)
\enskip \cdots
\end{gather*}
It can be extremely difficult to decide which rule to apply
and how, without some background knowledge about the objects and properties, as
there might be an infinite number of possibilities. This is one reason why it
is not possible in general to write a program which automatically infers
judgements that is guaranteed to find such a derivation if it exists.

\subsection{Derivable and  Admissible Rules}
Suppose we added the following rule to our definition of $\sansbold{M}$:
\[
\inferrule{\predm{s}{M}}{\pred{\mconc{((}\meta{s}\mconc{))}}{M}}(4)
\]
Does this change $\sansbold{M}$ in any way? Or, in other words, is there a 
string $s$ for which we can infer $\pred{s}{M}$ if we use $(4)$, but not
otherwise? 
This doesn't seem very likely: the rule just says that, if a string 
$\meta{s}$ is in $\sansbold{M}$, then it is permitted to add two pairs of matching 
parentheses to $\meta{s}$ and the resultant string will still be in
$\sansbold{M}$. 
Since we already have a rule which allows us to add one pair of parentheses, 
we can apply this rule twice and achieve the same effect: 
\[
\inferrule*[Right=(3)]{\inferrule*[Right=(3)]{\predm{s}{M}}{\pred{\mconc{(}\meta{s}\mconc{)}}{M}}}{\predc{\mconc{((}\meta{s}\mconc{))}}{M}}
\]
This means that Rule $(4)$ is \emph{derivable} from the existing
rules. Derivable rules are those rules that can be derived directly 
from existing rules.  Rules that do not change the language are called 
\emph{admissible} rules, and all derivable rules are admissible. 
Curiously, however, not all admissible rules are derivable, for example:
\[
\inferrule{\predc{\mconc{()}\meta{s}}{M}}{\predm{s}{M}}(5)
\]
Although Rule~$(5)$  does not introduce any new strings to $\sansbold{M}$, the rule
is also not derivable from any of the existing rules. 

\section{Rule Inversion and Case Distinction}
\todo{This part}
\section{Induction} 

Natural deduction by itself is sometimes not powerful enough. For example,
although we can see that the Rule $(5)$ is valid for every string $s$
in $\sansbold{M}$, we cannot show this by simply combining the existing rules. We will
therefore introduce another proof technique here, called \emph{induction}. You
will probably know induction over natural numbers and structural induction
from mathematics. Both are special cases of a more
general induction principle called \emph{rule induction}. 

A set of inference rules $\mathcal{R}$ defining a judgement $\sansbold{A}$ is 
called an \emph{inductive definition} of $\sansbold{A}$, if, for any $\meta{s}$, $\predm{s}{A}$ 
holds if and only if $\pred{s}{A}$ is derivable using the rules in $\mathcal{R}$.
Not all propositions or sets can be defined using inductive
definitions. For example, while natural numbers are one of the standard
examples for such sets, real numbers cannot be so easily defined inductively.

\subsection{Rule Induction}
Let us go back to our previous example judgement $\sansbold{M}$ of properly matched
parentheses. The rules $(1)$ to $(3)$ provide an inductive definition of $\sansbold{M}$: 
\begin{definition}[The language of matching parentheses $\sansbold{M}$]
\begin{gather*}
{\inferrule{\;}{\predc{\epsilon}{M}}(1)}\quad
{\inferrule{\predm{s_1}{M}\\\predm{s_2}{M}}{\predm{s_1s_2}{M}}(2)} \quad
{\inferrule{\predm{s}{M}}{\predc{\mconc{(}\meta{s}\mconc{)}}{M}}(3)}
\end{gather*}
\end{definition}
\noindent Now, let us assume we want to prove some property $\sansbold{P}$ of the strings
in $\sansbold{M}$. Specifically, we want to show that if $\predm{s}{M}$ then
$\predm{s}{P}$, for any string $\meta{s}$. Since we know the
complete set of rules that make up the definition of $\sansbold{M}$, we merely
need to show that:
\begin{itemize}
  \item $\predc{\epsilon}{P}$, our base case.
  \item $\predm{s_1}{M}$ and $\predm{s_2}{M}$, and the \emph{induction
      hypotheses}
    \begin{gather*} \inferrule*[Left=(IH-1)]{\predm{s_1}{M}}{\predm{s_1}{P}} \quad\inferrule*[Right=(IH-2)]{\predm{s_2}{M}}{\predm{s_2}{P}} \end{gather*} 
      implies $\predm{s_1 s_2}{P}$
  \item $\predm{s}{M}$ and the \emph{induction hypothesis}
        $$\inferrule*[Right=(IH)]{\predm{s}{M}}{\predm{s}{P}}$$  
      implies $\predc{\mconc{(}\meta{s}\mconc{)}}{P}$
\end{itemize}
These cases can be straightforwardly derived from the rules for $\sansbold{M}$.
Note that the induction hypothesis allows us to
assume something that looks like our overall proof goal for each
smaller substring. This, combined with our base case, allows us to show that, 
for \emph{every} possible string in $\sansbold{M}$, our property $\sansbold{P}$
holds. 

For example, let us show that all $\meta{s}$ in $\sansbold{M}$ have the same number of opening and
closing parentheses.
\begin{theorem}[\sansbold{M} matches parentheses] 
  If $\predm{s}{M}$ then \<open(\meta{s}) = close(\meta{s})\>\footnote{Where
    \<open(\meta{s})\> and \<close(\meta{s})\> are the number of opening and 
    closing parentheses in $\meta{s}$ respectively}.
  \begin{proof}  By rule induction with the rules of $\sansbold{M}$ on the premise ($\predm{s}{M}$)
\begin{description}
\item[Base case] (from 1) \\ We must show \<open(\conc{\epsilon}) = close
  (\conc{\epsilon})\>; trivial as \<open(\conc{\epsilon}) = 0 = close (\conc{\epsilon})\>.

\item[Inductive case] (from 2) \\ For $\predm{s_1}{M}$ and $\predm{s_2}{M}$,
  and the following induction hypotheses:
  \begin{enumerate}
    \item \<\predm{s_1}{M} \implies open(\meta{s_1}) = close (\meta{s_1})\>

    \item \<\predm{s_2}{M} \implies open(\meta{s_2}) = close (\meta{s_2})\>
  \end{enumerate}
  We must show that \<open(\meta{s_1s_2}) = close(\meta{s_1s_2})\>. 

    $\begin{array}{lclr}
    \mathit{open}(\meta{s_1 s_2}) & = & \mathit{open}(\meta{s_1}) + \mathit{open} (\meta{s_2}) & \\ 
                                  & = & \mathit{close}(\meta{s_1}) + \mathit{open} (\meta{s_2}) & \text{(Induction Hypothesis 1)}\\
                                  & = & \mathit{close}(\meta{s_1}) + \mathit{close} (\meta{s_2}) & \text{(Induction Hypothesis 2)}\\
                                  & = & \mathit{close} (\meta{s_1 + s_2})\> \end{array}$
  
\item[Inductive case] (from 3)\\
For $\predm{s}{M}$, with inductive hypothesis 
  \begin{itemize}
  \item \<\predm{s}{M} \implies open(\meta{s}) = close (\meta{s})\>
  \end{itemize}
  We must show that $\mathit{open}(\mconc{(}\meta{s}\mconc{)}) =
  \mathit{close}(\mconc{(}\meta{s}\mconc{)})$:

  $\begin{array}{lclr}
	\mathit{open}(\mconc{(}\meta{s}\mconc{)}) &=& \mathit{open}(\mconc{(}) + \mathit{open}(\meta{s}) + \mathit{open}(\mconc{)}) \\
                                                  &=& 1  + \mathit{open}(\meta{s}) + 0  \\
                                                  &=& 1  + \mathit{close}(\meta{s}) + 0 & \text{(Induction Hypothesis)} \\
	                                          &=& \mathit{close}(\mconc{(}) + \mathit{close}(\meta{s}) + \mathit{close}(\mconc{)})\\
					          &=& \mathit{close}(\mconc{(}\meta{s}\mconc{)}) 
\end{array}$
\end{description}
\end{proof}
\end{theorem}
\noindent In the above proof, we used the rules of arithmetic and assumed a sensible
definition of \<close\> and \<open\>, such as \<open(\mconc{(}) = 1\>, \<close
(\mconc{)}) = 1\>, \<open(\mconc{)}) = 0\>, and \<close(\mconc{(}) = 0\>. In a fully
formal proof, we would need a formal definition of these two
functions\footnote{We also need a formalisation of arithmetic, but this would make the proof overly formal
  and obscure understanding.}.

\subsection{Ambiguity}
The definition of $\sansbold{M}$, although correct, has a undesirable property: for
any string in $\sansbold{M}$, we do not have just one derivation, but an
infinite number of possible derivations, since any string $\meta{s}$ can be split
into  $\conc{\epsilon}$ and $\meta{s}$ by applying (2), then (1) to get rid of
 spurious $\conc{\epsilon}$ obligations (see the example given in
 Section~\ref{sec:Deductive}). This property is called \emph{ambiguity}. Ambiguous rules
 not only make it harder for humans to prove properties, but also make it
 much harder to derive an efficient algorithm to determine if a judgement holds. 

 Fortunately, if we take a more structured view on the elements of this
 language $\sansbold{M}$, we can come up with an alternative set of rules, where we have
 exactly one derivation for each string in the set. We can interpret each
 string as a possibly empty list ($\sansbold{L}$) of non-empty parenthesis expressions
 ($\sansbold{N}$) according to the following inference rules: 
 \begin{definition}[Unambiguous matching parentheses $\sansbold{L}
   \cup \sansbold{N}$]
\begin{gather*}
\inferrule*[Right=(L-1)]{\ }{\predc{\epsilon}{L}}\quad\quad\quad
\inferrule*[Right=(L-2)]{\predm{s_1}{N}\\\predm{s_2}{L}}{\predm{s_1s_2}{L}}\quad\quad\quad
\inferrule*[Right=(N-1)]{\predm{s}{L}}{\predm{\mconc{(}\meta{s}\mconc{)}}{N}}
\end{gather*}
\end{definition}
\noindent The interesting point here is that $\sansbold{L}$ and $\sansbold{N}$ are defined in terms of each
other: we have a mutually recursive definition. 

We can use this style of rule induction to show that every string in $\sansbold{M}$ is in
$\sansbold{L}$ --- one direction of the two required to show that $\sansbold{M}$
and $\sansbold{L}$ are equivalent. To do this, we first  need to show the
following lemma, which resembles rule (2), but with $\sansbold{M}$ replaced with $\sansbold{L}$: 
\begin{lemma}
  \label{lem:parenslemma}
If $\predm{s_1}{L}$ and $\predm{s_2}{L}$, then $\predm{s_1s_2}{L}$.
\begin{proof} By rule induction on the first premise ($\predm{s_1}{L}$)
  \begin{description}
    \item[Base case] (from L-1) \\
      If $\predm{s_2}{L}$ then $\predm{\conc{\epsilon}s_2}{L}$. Trivial as
      $\meta{\conc{\epsilon}s_2} = \meta{s_2}$.
    \item[Inductive case] (from L-2) \\
      Assuming $\predm{s_{11}}{N}$ and $\predm{s_{12}}{L}$, and the induction hypothesis:  
      \begin{gather*}
         \inferrule*[Right=(IH)]{\predm{s_{12}}{L}\\\predm{s_2}{L}}{\predm{s_{12}s_2}{L}}
        \end{gather*}
      We must show that if $\predm{s_2}{L}$ then $\predm{s_{11}s_{12}s_2}{L}$:
      \begin{displaymath}
        \inferrule*[Right=(L-2)]
                   {\inferrule*{\ }{\predm{s_{11}}{N} } \\
                     \inferrule*[Right=(IH)]{\inferrule*{\ }{\predm{s_{12}}{L}}\\\inferrule*{\ }{\predm{s_2}{L}} }{\predm{s_{12}s_2}{L}}}
                   {\predm{s_{11}s_{12}s_2}{L}}
      \end{displaymath}
\end{description}
This is sufficient to prove the lemma as we only need to examine cases arising
from the rules that define $\sansbold{L}$.
\end{proof} 
\end{lemma}

\noindent With this lemma, proving that $\sansbold{M}$ is contained in $\sansbold{L}$ is a
reasonably straightforward rule induction:

\begin{theorem}[$\sansbold{M} \subseteq \sansbold{L}$] If $\predm{s}{M}$ then $\predm{s}{L}$.
\begin{proof} By rule induction with the
  rules of $\sansbold{M}$. 
\begin{description}
  \item[Base case] (from 1) \\
    We must show $\predc{\epsilon}{L}$; trivially by
    the axiom (L-1).
  \item[Inductive Case] (from 2) \\
    For $\predm{s_1}{M}$ and $\predm{s_2}{M}$, we have induction hypotheses:
    \begin{gather*}
      \inferrule*[Left=(IH-1)]{\predm{s_1}{M}}{\predm{s_1}{L}}\quad
      \inferrule*[Right=(IH-2)]{\predm{s_2}{M}}{\predm{s_2}{L}}
    \end{gather*}
    We must show that $\predm{s_1s_2}{L}$; achieved straightforwardly using 
    the lemma we proved earlier:
    \begin{displaymath}
      \inferrule*[Right=Lemma~\ref{lem:parenslemma}]{\inferrule*[Left=(IH-1)]{\inferrule*{\
          }{\predm{s_1}{M}}}{\predm{s_1}{L}} \\ \inferrule*[Right=(IH-2)]{\inferrule*{\
          }{\predm{s_2}{M}}}{\predm{s_2}{L}} }{\predm{s_1s_2}{L}}
      \end{displaymath}
  \item[Inductive Case] (from 3) \\
    For a string $\predm{s}{M}$, we must show that
    $\predm{\mconc{(}s\mconc{)}}{L}$ with the induction hypothesis:
    \begin{displaymath}
      \inferrule*[Right=(IH)]{\predm{s}{M}}{\predm{s}{L}}
    \end{displaymath}
    To do this, we exploit the fact that $\mconc{(}\meta{s}\mconc{)} = \mconc{(}\meta{s}\mconc{)}\conc{\epsilon}$:
    \begin{displaymath}
      \inferrule*[Right=(L-2)]{\inferrule*[Left=(N-1)]{\inferrule*[Left=(IH)]{\inferrule*{\ }{\predm{s}{M}}
          }{\predm{s}{L}} }{\predm{\mconc{(}s\mconc{)}}{N}} \\
        \inferrule*[Right=(L-1)]{\ }{\predc{\epsilon}{L}}}{\predm{\mconc{(}s\mconc{)}}{L}}
    \end{displaymath} \end{description}
\end{proof}
\end{theorem}

\subsection{Simultaneous Induction}

If we try and prove the other direction of the equivalence, that
$\sansbold{L}$ is contained within $\sansbold{M}$, we end up with a troubling
inductive case. Knowing $\predm{s_1}{N}$ and $\pred{s_2}{L}$, and the following
inductive hypothesis,
\begin{gather*}
\inferrule*[Right=(IH)]{\predm{s_2}{L}}{\predm{s_2}{M}}
\end{gather*}
we must show that $\predm{s_1s_2}{M}$. This is quite troublesome, as there is no inductive hypothesis that deals with
$\meta{s_1}$, as it is a string in $\sansbold{N}$, not $\sansbold{L}$:

\begin{displaymath}
\inferrule*[Right=(2)]{\inferrule*{???}{\predm{s_1}{M}} \\
  \inferrule*[Right=IH]{\inferrule*{\ }{\predm{s_2}{L}}}{\predm{s_2}{M}}}{\predm{s_1s_2}{M}}
\end{displaymath}
 
\noindent To prove this direction of the equivalence, we must first \emph{generalise} our goal. Rather than prove that,
for any $\meta{s}$, $\predm{s}{L}$ implies $\predm{s}{M}$; we prove the
seemingly stronger claim that, for any $\meta{s}$, $\predm{s}{L}$ \emph{or}
$\predm{s}{N}$ implies $\predm{s}{M}$. While this results in an additional
obligation to prove (arising from the rule N-1), it also equips us with more inductive
hypotheses, which makes it possible to solve the above dilemma.

\begin{theorem}[$\sansbold{L} \cup \sansbold{N} \subseteq \sansbold{M}$] 
For any $\meta{s}$, $\predm{s}{L}$ \emph{or} $\predm{s}{N}$ implies $\predm{s}{M}$.
 \begin{proof} By simultaneous rule induction with the rules of $\sansbold{L}$
   and $\sansbold{M}$.
   \begin{description}
   \item[Base case] (from L-1) \\
     We must show that $\predc{\epsilon}{M}$; trivial by rule (1).
   \item[Inductive case] (from L-2) \\
     For $\predm{s_1}{N}$ and $\predm{s_2}{L}$, with inductive hypotheses:
     \begin{gather*}
       \inferrule*[Left=(IH-1)]{\predm{s_1}{N}}{\predm{s_1}{M}}\quad
       \inferrule*[Right=(IH-2)]{\predm{s_2}{L}}{\predm{s_2}{M}}
     \end{gather*}
     We must show that $\predm{s_1s_2}{M}$:
     $$ \inferrule*[Right=(2)]{ \inferrule*[Left=(IH-1)]{\inferrule*{\
         }{\predm{s_1}{N}}}{\predm{s_1}{M}} \\ \inferrule*[Right=(IH-2)]{\inferrule*{\
         }{\predm{s_2}{L}}}{\predm{s_2}{M}}}{\predm{s_1s_2}{M}} $$ 
   \item[Inductive case] (from N-1) \\
     For $\predm{s}{L}$, with the inductive hypothesis:
     \begin{gather*}
        \inferrule*[Right=(IH)]{\predm{s}{L}}{\predm{s}{M}}
     \end{gather*}
     We must show $\predm{\mconc{(}s\mconc{)}}{M}$:
     $$ \inferrule*[Right=(3)]{\inferrule*[Right=(IH)]{\inferrule*{\ }{\predm{s}{L}}}{\predm{s}{M}}}{\predm{\mconc{(}s\mconc{)}}{M}} $$
   \end{description}
\end{proof} 
\end{theorem}


\section{Arithmetic Expressions and EBNF}


% Let us look at one more example of an ambiguous definition: simple
% arithmetic expressions, given here both in EBNF form and as inductive
% definition using inference rules:
% 
% The EBNF
%   \begin{grammar}
%      \grule{Expr}{%
%        \gnterm{int}\ \galt\ (\gnterm{Expr}) 
%        \galt\ {\gnterm{Expr} + \gnterm{Expr}\ 
%        \galt\ {\gnterm{Expr} * \gnterm{Expr}}}}
%    \end{grammar}
%  describes the same language as the following set of inference rules ($\gnm{int}$  represents an integer constant):
%   \begin{gather*}
% \inferrule{\pred{e}{\gnm{Expr}}}{\pred{(e)}{\gnm{Expr}}}\qquad
% \inferrule{\predand{e_1}{\gnm{Expr}} \pred{e_2}{\gnm{Expr}}}{\pred{e_1\;\texttt{+}\;\;e_2}{\gnm{Expr}}}
% \qquad
% \inferrule{\predand{e_1}{\gnm{Expr}}\pred{e_2}{\gnm{Expr}}}{\pred{e_1\;\texttt{*}\;\;e_2}{\gnm{Expr}}}
%   \end{gather*}
% Although in this case, there are not an infinite number of possible
% derivations for each expression, every expression which contains more than a
% single arithmetic operation still can be derived in more than one way:
% \begin{gather*}
%     \inferrule{\inferrule{\inferrule{}{1\; \gnm{Expr}}\quad
%         \inferrule{}{2\; \gnm{Expr}}}{1 + 2\; \gnm{Expr}}\;\;
%       \inferrule{}{3\;\gnm{Expr}}}{%
%         1+2*3\;\gnm{Expr}}
%   \end{gather*}
% \begin{gather*}
%     \inferrule{
%       \inferrule{}{1\;\gnm{Expr}}\;\;
% \inferrule{\inferrule{}{2\; \gnm{Expr}}\quad
%         \inferrule{}{3\; \gnm{Expr}}}{2 * 3\; \gnm{Expr}}
%     }{%
%         1+2*3\;\gnm{Expr}}
%   \end{gather*}
% 
%   Although both derivations are correct with respect to the rules given, the
%   second derivation is more appropriate for an arithmetic expression, as it
%   decomposes the expression first into two summands. We give an alternative
%   definition, which takes precedence and associativity  of the operators into
%   account:
%   
%   \begin{gather*}
% \inferrule{\predand{e_1}{\gnm{SExpr}}\pred{e_2}{\gnm{PExpr}}}{\pred{e_1\;\;\texttt{+}\;e_2}{\gnm{SExpr}}}
% \qquad
% \inferrule{\pred{e}{\gnm{PExpr}}}{\pred{e}{\gnm{SExpr}}}
% \end{gather*}
% 
% \begin{gather*}
% \inferrule{\predand{e_1}{\gnm{PExpr}}\pred{e_2}{\gnm{FExpr}}}{\pred{e_1\;\texttt{*}\;\;e_2}{\gnm{PExpr}}}
% \qquad
% \inferrule{e\; FExpr}{e\;PExpr}
%   \end{gather*}
% 
% 
% \begin{gather*}
% \inferrule{e\; SExpr}{\texttt{(}e\texttt{)}\;FExpr}\qquad
% \inferrule{}{int\;FExpr}
%   \end{gather*}
% The unambiguous grammar is, again, much more complicated than the original
% grammar, even for such a simple language. This is not surprising, as it
% contains additional structural information. For programming languages, ambiguous
% grammars are problematic, as they may allow different interpretations of
% programs, and are therefore usually avoided. 
% 
% \subsection{Simultaneous Induction}
% How can we apply the principle of rule induction to mutually recursive
% definitions like those of $L$ and \textit{SExpr}? In most cases, we have to
% generalise the proof goal. For example, if we want to prove a property $P$ for all
% $e$ in \textit{SExpr}, that is \<\pred{e}{SExpr}\> implies \<\pred{e}{PExpr}\>.
% By the principle of rule induction we have to show that 
% \begin{itemize}
% \item under the assumption that
% \begin{itemize}
%  \item[-] \<e = e_1 + e_2\>
%  \item[-] \<\pred{e_1}{SExpr}\>
%  \item[-] \<\pred{e_2}{PExpr}\>
%  \end{itemize}
%  and the Induction Hypothesis 
% \begin{itemize}
%   \item[-]  \<\pred{e_1}{P}\>
%   \end{itemize}
%   show that   \<\pred{e_1+e_2}{P}\>
% \item under the assumption that
%   \begin{itemize}
%   \item[-] \<\pred{e}{PExpr}\>
%   \end{itemize}
%   show that \<\pred{e}{P}\>
% \end{itemize} 
% 
% The problem is that, since we only try to show something about
% \<SExpr\>, the induction hypothesis does not say anything about \<e_2\> (we
% only know that \<\pred{e_2}{PExpr}\>, and nothing at all about $e$ in for the
% second rule. In most cases, this is not enough to prove anything. The
% solution is often to try and prove a more general statement instead which
% leads to stronger induction hypothesis. If we try to show, for instance, that 
% \<\pred{e}{SExpr}\> or \<\pred{e}{PExpr}\> or \<\pred{e}{FExpr}\> implies \<\pred{e}{P}\>,
% we have more cases to cover on one hand (one for each inference rule which has
% \textit{PExpr}, \textit{SExpr} or \textit{FExpr} in the conclusion), on the other hand the induction
% hypothesis cover all the premises in the rules:
% \begin{itemize}
% \item under the assumption that
% \begin{itemize}
%  \item[-] \<e = e_1 + e_2\>
%  \item[-] \<\pred{e_1}{SExpr}\>
%  \item[-] \<\pred{e_2}{PExpr}\>
%  \end{itemize}
%  and the Induction Hypothesis 
% \begin{itemize}
%   \item[-]  \<\pred{e_1}{P}\>
%   \item[-]  \<\pred{e_2}{P}\>
%   \end{itemize}
%   show that   \<\pred{e_1+e_2}{P}\>
% \item under the assumption that
%   \begin{itemize}
%   \item[-] \<\pred{e}{PExpr}\>
%   \end{itemize}
%  and the Induction Hypothesis 
%  \begin{itemize}
%  \item[-]  \<\pred{e}{P}\>
%  \end{itemize}
%  show that \<\pred{e}{P}\> (trivial)
% \item under the assumption that
% \begin{itemize}
%  \item[-] \<e = e_1 * e_2\>
%  \item[-] \<\pred{e_1}{SExpr}\>
%  \item[-] \<\pred{e_2}{PExpr}\>
%  \end{itemize}
%  and the Induction Hypothesis 
% \begin{itemize}
%   \item[-]  \<\pred{e_1}{P}\>
%   \item[-]  \<\pred{e_2}{P}\>
%   \end{itemize}
%   show that   \<\pred{e_1*e_2}{P}\>
%   
% \item \ldots
% \end{itemize} 
% 
% 
% \section{Judgements and Relations}
% So far, we defined a judgement to be a statement about a property of an
% object. We can generalise this definition to relation between a number of
% objects. Consider the following inductive definition of the  relation ``a divides b'' on natural
% numbers. For convenience reasons, we choose an infix notation here:
%   \begin{gather*}
% \inferrule{}{0\;div\; n}\\\phantom{a}\\
% \inferrule{n\; div\; m}{n\; div\; (m+n)}
% \end{gather*}
% As before, we can also view this as an inductive definition of a set. In this
% case, a set of pairs, where $(a,b)$ in \textit{div} if and only if $a$ divides
% $b$. Rule inductions works exactly in the same way as in the unary case.
% 
% \section{Boolean Expression Example}		
% As another example, consider boolean expressions. For simplicity, we only include three operators for now: $\wedge$, $\vee$, and $\neg$, the constants \textit{True} and \textit{False}, and parentheses. Our first attempt at defining a set of inference rules to characterise boolean expressions might look as follows:
% 
%   \begin{gather*}
% 	\inferrule{\phantom{bla}}{\pred{\bf{True}}{\gnm{BExpr}}}\qquad \inferrule{\phantom{bla}}{\pred{\bf{False}}{\gnm{BExpr}}}\qquad \inferrule{\pred{e}{\gnm{BExpr}}}{\neg\pred{e}{\gnm{BExpr}}}\\ \\
% \inferrule{\pred{e}{\gnm{BExpr}}}{\pred{(e)}{\gnm{BExpr}}}\qquad
% \inferrule{\predand{e_1}{\gnm{BExpr}} \pred{e_2}{\gnm{BExpr}}}{\pred{e_1 \wedge e_2}{\gnm{BExpr}}}
% \qquad
% \inferrule{\predand{e_1}{\gnm{BExpr}}\pred{e_2}{\gnm{BExpr}}}{\pred{e_1\vee e_2}{\gnm{BExpr}}}
%   \end{gather*}
% 
% Unfortunately, with this set of rules, we have the same problem we had with our rules for arithmetic expressions. Even though they inductively define the set of boolean expressions, they are ambiguous and do not reflect  associativity and precedence of the operators. So, we need to come up with an alternative definition. The operator $\neg$ has the highest precedence, $\vee$ the lowest, and both $\wedge$ and $\vee$ are left associative. The solution is also similar to the solution for arithmetic expressions. First, we need rules to define the subset of boolean expressions  which can be arguments of the operator with the highest precedence, negation. These can only be atomic expressions (constants), any expression in parentheses, or such an expression preceded by negation. Let's call this subset \textit{NbExpr}. We call the boolean expressions we generate with the new rules \textit{Bexpr}.
%   \begin{gather*}
% 	\inferrule{\phantom{bla}}{\pred{\bf{True}}{\gnm{Nbexpr}}}\qquad \inferrule{\phantom{bla}}{\pred{\bf{False}}{\gnm{Nbexpr}}}\qquad \inferrule{\pred{e}{\gnm{Bexpr}}}{\neg\pred{(e)}{\gnm{Nbexpr}}}\\ \\
%   \end{gather*}
% The rules for the operators $\wedge$ and $\vee$ correspond to those for addition and multiplication. Since the operators are left associative, the expression on the left hand side can only be an expression with stronger cohesion than the one on the right hand side. For the $\wedge$ operator, it has to be a $Nbexpr$.
% \begin{gather*}
% 	\inferrule{\predand{e_1}{\gnm{NbExpr}} \pred{e_2}{\gnm{Abexpr}}}{\pred{e_1\wedge e_2}{\gnm{Abexpr}}}
% 	\qquad
% 	\inferrule{\predand{e_1}{\gnm{Abexpr}}\pred{e_2}{\gnm{Bexpr}}}{\pred{e_1\vee e_2}{\gnm{Bexpr}}}
% \end{gather*}	
% And finally we need rules to express the fact the $Nbexpr \subseteq Abexpr \subseteq Bexpr$
% \begin{gather*}
% 	\inferrule{\pred{e}{\gnm{Nbexpr}}}{\pred{e}{\gnm{Abexpr}}}
% 	\qquad
% 	\inferrule{\pred{e}{\gnm{Abexpr}}}{\pred{e}{\gnm{Bexpr}}}
% \end{gather*}	


\end{document}